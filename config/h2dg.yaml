gpu_index: 0

# Data configuration
data:
  datamodule_type: "HANCOCK_HierarchicalDirectedSurvivalGraph"
  data_root: "./data/HANCOCK"
  
  # Model paths
  path_optimus: "./data/models/H-optimus-1"
  path_lm: "./data/models/Bio_ClinicalBERT"
  path_kimianet: "./data/models/kimia_net.pth"
  
  # Tokenization parameters
  max_tokens_history: 200
  max_tokens_surgery: 512
  max_tokens_report: 60
  padding: 'max_length'
  truncation: True
  
  # Time discretization
  num_time_bins: 100  # Discretize survival time into 100 bins
  
  # Folds
  k: 5
  fold: 1

  aggregate_images: True

# Model configuration
model:
  name: "hierarchical_directed_survival_gnn_logistic_hazard"
  args:
    hidden_dim: 512                     # Hidden dimension for GNN layers
    num_heads: 4                        # Number of attention heads for GATv2Conv
    dropout: 0.35                        # Dropout rate
    freeze_text_encoder: True           # Freeze BioBERT weights
    # Hazard head configuration
    hidden_reduction_factors: [2, 4, 6]

# Training configuration
training:
  # Core training parameters
  batch_size: 8             
  epochs: 100
  num_workers: 4
  seed: 42
  
  # Lightning Module configuration (for LogisticHazardModule)
  lightning_module_args:
    T_max: 3650              # T-max for survival function grid (10 years)
    scheme: "equidistant"    # Time discretization scheme: "equidistant" or "quantiles"
    lambda_xcal: 7.0         # X-CAL calibration penalty (0 = disabled)
    xcal_nbins: 28           # Number of bins for X-CAL
    lambda_ranking: 0.0      # Ranking loss weight (0 = disabled)
    ranking_margin: 0.0      # Margin for ranking loss
    temperature: 1.5         # Temperature scaling (>1.0 = softer predictions)
  
  # Optimizer configuration
  optimizer:
    optimizer_name: "AdamW"
    learning_rate: 0.0003    # Lower LR for graph-based models
    weight_decay: 0.0005
  
  # Scheduler configuration
  scheduler:
    scheduler_name: "CosineAnnealingLR"
    scheduler_eta_min: 0.00001
    scheduler_T_max: 100
  
  # Checkpoint configuration
  checkpoints:
    checkpoint_dir: "results/h2dg/contrib"
    patience: 10             
    delta: 0.0
    save_interval: 1
    verbose: False
  
  # Learning statistics configuration
  learning_stat:
    main_metric: "ibs"


